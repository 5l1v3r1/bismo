{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f = open(\"main.cpp\",\"w+\")\n",
    "wgtIdx = 0\n",
    "thIdx = 0\n",
    "act_name = \"bin_img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNNBipolarThresholdingLayer\n",
      "QNNBipolarThresholdingLayer\n",
      "QNNBipolarThresholdingLayer\n",
      "(256, 784)\n",
      "(1, 256)\n",
      "(1, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load image using PIL\n",
    "img = Image.open(\"7.png\")\n",
    "# convert to black and white\n",
    "img = img.convert(\"L\")\n",
    "# convert to numpy array\n",
    "img = np.asarray(img)\n",
    "# display\n",
    "% matplotlib inline\n",
    "#plt.imshow(img, cmap='gray')\n",
    "\n",
    "from QNN.layers import *\n",
    "import pickle\n",
    "# load the qnn\n",
    "qnn = pickle.load(open(\"mnist-w1a1.pickle\", \"rb\"))\n",
    "new = []\n",
    "prev_name = ''\n",
    "prev = qnn[0]\n",
    "#the new list will have just pairs fullyconnected-thresholding\n",
    "for l in qnn:\n",
    "    if(prev_name == 'QNNFullyConnectedLayer'and l.layerType() == 'QNNBipolarThresholdingLayer'):\n",
    "        new.append(prev)\n",
    "        new.append(l)\n",
    "        print(l.layerType())\n",
    "    prev_name = l.layerType()\n",
    "    prev = l\n",
    "new\n",
    "\n",
    "print(new[0].W.shape)\n",
    "print(new[1].thresholds.shape)\n",
    "predict(qnn[0:1], img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write(\"\\\n",
    "#include \\\"BitSerialMatMulAccelDriver.hpp\\\"\\n\\\n",
    "#include \\\"myfancyNewHeaderFile.hpp\\\"\\n\\\n",
    "\\n\\\n",
    "int main(int argc, char * argv[]){\\n\\\n",
    "\\\n",
    "}\\n\\\n",
    "\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include \"BitSerialMatMulAccelDriver.hpp\"\n",
      "#include \"myfancyNewHeaderFile.hpp\"\n",
      "#include \"cnpy.h\"\n",
      "#include <vector>\n",
      "#include <iostream>\n",
      "#include <assert.h>\n",
      "#include \"utils.hpp\"\n",
      "using namespace gemmbitserial;\n",
      "\n",
      "int main(int argc, char * argv[]){\n",
      "  try {\n",
      "    WrapperRegDriver * platform = initPlatform();\n",
      "    BitSerialMatMulAccelDriver * acc_driver = new BitSerialMatMulAccelDriver(platform);\n",
      "    \n",
      "\\n    //load activations from file\n",
      "    cnpy::NpyArray activations_npy = cnpy::npy_load(\"bin_img.npy\");\n",
      "    uint8_t * activations_ptr = activations_npy.data<uint8_t>();\n",
      "    size_t act_size = activations_npy.shape.size();\n",
      "    assert(act_size == 2);\n",
      "    size_t rows_rhs = activations_npy.shape[0];\n",
      "    size_t cols_rhs = activations_npy.shape[1];\n",
      "    size_t act_bytes = rows_rhs * cols_rhs;\n",
      "    \n",
      "\n",
      "    //for each test layer (pair fully connected-thresholding) \n",
      "\n",
      "    //weights loading  \n",
      "        cnpy::NpyArray weights_npy = cnpy::npy_load(\"weights_l0.npy\");\n",
      "        uint8_t* weights_ptr = weights_npy.data<uint8_t>();\n",
      "        int32_t wshape_size = weights_npy.shape.size();\n",
      "        assert(wshape_size == 2);\n",
      "        std::vector<size_t> wshape;\n",
      "        int32_t wgt_bytes = 1;\n",
      "        for(int i = 0; i < wshape_size; i++){\n",
      "            wshape.push_back(weights_npy.shape[i]);\n",
      "            wgt_bytes *= weights_npy.shape[i];\n",
      "        }\n",
      "\n",
      "        //prepare gemm bit serial\n",
      "        size_t rows_lhs = wshape[0];\n",
      "        size_t cols_lhs = wshape[1];\n",
      "        //COMMENT: the vector creation is not neeeded, could manipulate directly pointer\n",
      "        uint8_t * lhs = new uint8_t[wgt_bytes];\n",
      "        uint8_t * rhs = new uint8_t[act_bytes];\n",
      "        memcpy(lhs,weights_ptr, rows_lhs * cols_lhs * sizeof(uint8_t));\n",
      "        memcpy(rhs, activations_ptr, rows_rhs * cols_rhs * sizeof(uint8_t));\n",
      "        uint8_t * rhs_t = new uint8_t[act_bytes];\n",
      "        //Be compliant with gemmbitserial hypothesis that rhs always transposed\n",
      "        transpose(activations_ptr, rows_rhs, cols_rhs, rhs_t, cols_rhs, rows_rhs);\n",
      "        //Allocate gemmbitserial\n",
      "        //TODO still miss bits and sign, now no check on matrices sizes\n",
      "        //p2S?\n",
      "        GEMMContext ctx = acc->allocGEMMContext(\n",
      "            rows_lhs, ncols, rows_rhs, nbits_lhs, nbits_rhs, sgn_lhs, sgn_rhs\n",
      "        );\n",
      "        ctx.lhs.importRegular(lhs);\n",
      "        ctx.rhs.importRegular(rhs);\n",
      "\n",
      "        //Allocate buffer for lhs and rhs\n",
      "        //TODO: fixed size a priori?(now nope)\n",
      "        size_t res_bytes = rows_lhs * rows_rhs * sizeof(uint8_t);\n",
      "        void * lhs_base_addr = platform->allocAccelBuffer(wgt_bytes);\n",
      "        void * rhs_base_addr = platform->allocAccelBuffer(act_bytes);\n",
      "        void * res_base_addr = platform->allocAccelBuffer(res_bytes);\n",
      "        platform->copyBufferHostAccel(ctx.lhs, lhs_base_addr, wgtbytes);\n",
      "        //TODO: Invalidate cache?\n",
      "        platform->copyBufferHostAccel(ctx.rgs, rhs_base_addr, act_bytes);\n",
      "\n",
      "    //thresholds loading \n",
      "        cnpy::NpyArray ths_npy = cnpy::npy_load(\"ths_l0.npy\");\n",
      "        int32_t * ths_ptr = ths_npy.data<int32_t>();\n",
      "        size_t thshape_size = ths_npy.shape.size();\n",
      "        // which layout? ths[thnmbr][rows_lhs]? ths[rows_lhs][thnumber]? \n",
      "        // the thresholding stage is in the second layout\n",
      "        assert(thshape_size == 2);\n",
      "        //loading ths\n",
      "        //iteration depends on the layout\n",
      "        // loop on(follow an example) acc_driver->thsSingleTransfer(&ths[i * ncols_ths + j], addr_offset , i, j);\n",
      "        //addr_offset bram address, i-j determine which is the target bram among the possible i-j\n",
      "        //This loading should also take care of possible folding in the different dimensions\n",
      "        int32_t addr_offset = 0;\n",
      "        int32_t dpa_lhs = acc_driver->getdpaDimLHS();\n",
      "        for (int i = 0; i < rows_lhs; i++){\n",
      "            for(int j = 0; j < thnumber; j++ ){\n",
      "                acc_driver->thsSingleTransfer(&ths_bs[i * thnumber + j], addr_offset , i, j);\n",
      "            }\n",
      "            if((i+1)%dpa_lhs == 0){\n",
      "            addr_offset ++;\n",
      "            }\n",
      "          }\n",
      "\n",
      "        // Act, Weights, Thresholds all load offload to accel\n",
      "        offloadMVT_Preloaded(weights, wgtprec, activations, actprec, thresholds, thnumber, res_base_addr);\n",
      "        // golden results computation\n",
      "        int32_t * qres = new int32_t[res_bytes]\n",
      "        gemmBitSerial(ctx);\n",
      "        quantizeMatrix(ctx.res, ths, rows_lhs, rows_rhs, rows_lhs, thnmbr, qres, 0);\n",
      "        uint8_t * hw_res = new uint8_t[res_bytes]\n",
      "        //we should take care of which output is produced uint8_t/int32_t...\n",
      "        platform->copyBufferAccelToHost(res_base_addr, hw_res, res_bytes)\n",
      "        memcmp(hw_res, qres, res_bytes);\n",
      "    }\n",
      "    platform->deallocAccelBuffer(lhs_base_addr);\n",
      "    platform->deallocAccelBuffer(rhs_base_addr);\n",
      "    platform->deallocAccelBuffer(res_base_addr);\n",
      "    delete acc_driver;\n",
      "    deinitPlatform(platform);\n",
      "  } catch (const char * e) {\n",
      "    cout << \"Exception: \" << e << endl;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\n",
    "#include \\\"BitSerialMatMulAccelDriver.hpp\\\"\\n\\\n",
    "#include \\\"myfancyNewHeaderFile.hpp\\\"\\n\\\n",
    "#include \\\"cnpy.h\\\"\\n\\\n",
    "#include <vector>\\n\\\n",
    "#include <iostream>\\n\\\n",
    "#include <assert.h>\\n\\\n",
    "#include \\\"utils.hpp\\\"\\n\\\n",
    "using namespace gemmbitserial;\\n\\\n",
    "\\n\\\n",
    "int main(int argc, char * argv[]){\\n\\\n",
    "  try {\\n\\\n",
    "    WrapperRegDriver * platform = initPlatform();\\n\\\n",
    "    BitSerialMatMulAccelDriver * acc_driver = new BitSerialMatMulAccelDriver(platform);\\n\\\n",
    "    \\n\\\\n\\\n",
    "    //load activations from file\\n\\\n",
    "    cnpy::NpyArray activations_npy = cnpy::npy_load(\\\"\"+ str(act_name) +\".npy\\\");\\n\\\n",
    "    uint8_t * activations_ptr = activations_npy.data<uint8_t>();\\n\\\n",
    "    size_t act_size = activations_npy.shape.size();\\n\\\n",
    "    assert(act_size == 2);\\n\\\n",
    "    size_t rows_rhs = activations_npy.shape[0];\\n\\\n",
    "    size_t cols_rhs = activations_npy.shape[1];\\n\\\n",
    "    size_t act_bytes = rows_rhs * cols_rhs;\\n\\\n",
    "    \\n\\n\\\n",
    "    //for each test layer (pair fully connected-thresholding) \\n\\n\\\n",
    "    //weights loading  \\n\\\n",
    "        cnpy::NpyArray weights_npy = cnpy::npy_load(\\\"weights_l\"+ str(wgtIdx)+\".npy\\\");\\n\\\n",
    "        uint8_t* weights_ptr = weights_npy.data<uint8_t>();\\n\\\n",
    "        int32_t wshape_size = weights_npy.shape.size();\\n\\\n",
    "        assert(wshape_size == 2);\\n\\\n",
    "        std::vector<size_t> wshape;\\n\\\n",
    "        int32_t wgt_bytes = 1;\\n\\\n",
    "        for(int i = 0; i < wshape_size; i++){\\n\\\n",
    "            wshape.push_back(weights_npy.shape[i]);\\n\\\n",
    "            wgt_bytes *= weights_npy.shape[i];\\n\\\n",
    "        }\\n\\n\\\n",
    "        //prepare gemm bit serial\\n\\\n",
    "        size_t rows_lhs = wshape[0];\\n\\\n",
    "        size_t cols_lhs = wshape[1];\\n\\\n",
    "        //COMMENT: the vector creation is not neeeded, could manipulate directly pointer\\n\\\n",
    "        uint8_t * lhs = new uint8_t[wgt_bytes];\\n\\\n",
    "        uint8_t * rhs = new uint8_t[act_bytes];\\n\\\n",
    "        memcpy(lhs,weights_ptr, rows_lhs * cols_lhs * sizeof(uint8_t));\\n\\\n",
    "        memcpy(rhs, activations_ptr, rows_rhs * cols_rhs * sizeof(uint8_t));\\n\\\n",
    "        uint8_t * rhs_t = new uint8_t[act_bytes];\\n\\\n",
    "        //Be compliant with gemmbitserial hypothesis that rhs always transposed\\n\\\n",
    "        transpose(activations_ptr, rows_rhs, cols_rhs, rhs_t, cols_rhs, rows_rhs);\\n\\\n",
    "        //Allocate gemmbitserial\\n\\\n",
    "        //TODO still miss bits and sign, now no check on matrices sizes\\n\\\n",
    "        //p2S?\\n\\\n",
    "        GEMMContext ctx = acc->allocGEMMContext(\\n\\\n",
    "            rows_lhs, ncols, rows_rhs, nbits_lhs, nbits_rhs, sgn_lhs, sgn_rhs\\n\\\n",
    "        );\\n\\\n",
    "        ctx.lhs.importRegular(lhs);\\n\\\n",
    "        ctx.rhs.importRegular(rhs);\\n\\n\\\n",
    "        //Allocate buffer for lhs and rhs\\n\\\n",
    "        //TODO: fixed size a priori?(now nope)\\n\\\n",
    "        size_t res_bytes = rows_lhs * rows_rhs * sizeof(uint8_t);\\n\\\n",
    "        void * lhs_base_addr = platform->allocAccelBuffer(wgt_bytes);\\n\\\n",
    "        void * rhs_base_addr = platform->allocAccelBuffer(act_bytes);\\n\\\n",
    "        void * res_base_addr = platform->allocAccelBuffer(res_bytes);\\n\\\n",
    "        platform->copyBufferHostAccel(ctx.lhs, lhs_base_addr, wgtbytes);\\n\\\n",
    "        //TODO: Invalidate cache?\\n\\\n",
    "        platform->copyBufferHostAccel(ctx.rgs, rhs_base_addr, act_bytes);\\n\\n\\\n",
    "    //thresholds loading \\n\\\n",
    "        cnpy::NpyArray ths_npy = cnpy::npy_load(\\\"ths_l\"+ str(thIdx)+\".npy\\\");\\n\\\n",
    "        int32_t * ths_ptr = ths_npy.data<int32_t>();\\n\\\n",
    "        size_t thshape_size = ths_npy.shape.size();\\n\\\n",
    "        // which layout? ths[thnmbr][rows_lhs]? ths[rows_lhs][thnumber]? \\n\\\n",
    "        // the thresholding stage is in the second layout\\n\\\n",
    "        assert(thshape_size == 2);\\n\\\n",
    "        //loading ths\\n\\\n",
    "        //iteration depends on the layout\\n\\\n",
    "        // loop on(follow an example) acc_driver->thsSingleTransfer(&ths[i * ncols_ths + j], addr_offset , i, j);\\n\\\n",
    "        //addr_offset bram address, i-j determine which is the target bram among the possible i-j\\n\\\n",
    "        //This loading should also take care of possible folding in the different dimensions\\n\\\n",
    "        int32_t addr_offset = 0;\\n\\\n",
    "        int32_t dpa_lhs = acc_driver->getdpaDimLHS();\\n\\\n",
    "        for (int i = 0; i < rows_lhs; i++){\\n\\\n",
    "            for(int j = 0; j < thnumber; j++ ){\\n\\\n",
    "                acc_driver->thsSingleTransfer(&ths_bs[i * thnumber + j], addr_offset , i, j);\\n\\\n",
    "            }\\n\\\n",
    "            if((i+1)%dpa_lhs == 0){\\n\\\n",
    "            addr_offset ++;\\n\\\n",
    "            }\\n\\\n",
    "          }\\n\\n\\\n",
    "        // Act, Weights, Thresholds all load offload to accel\\n\\\n",
    "        offloadMVT_Preloaded(weights, wgtprec, activations, actprec, thresholds, thnumber, res_base_addr);\\n\\\n",
    "        // golden results computation\\n\\\n",
    "        int32_t * qres = new int32_t[res_bytes]\\n\\\n",
    "        gemmBitSerial(ctx);\\n\\\n",
    "        quantizeMatrix(ctx.res, ths, rows_lhs, rows_rhs, rows_lhs, thnmbr, qres, 0);\\n\\\n",
    "        uint8_t * hw_res = new uint8_t[res_bytes]\\n\\\n",
    "        //we should take care of which output is produced uint8_t/int32_t...\\n\\\n",
    "        platform->copyBufferAccelToHost(res_base_addr, hw_res, res_bytes)\\n\\\n",
    "        memcmp(hw_res, qres, res_bytes);\\n\\\n",
    "    }\\n\\\n",
    "    platform->deallocAccelBuffer(lhs_base_addr);\\n\\\n",
    "    platform->deallocAccelBuffer(rhs_base_addr);\\n\\\n",
    "    platform->deallocAccelBuffer(res_base_addr);\\n\\\n",
    "    delete acc_driver;\\n\\\n",
    "    deinitPlatform(platform);\\n\\\n",
    "  } catch (const char * e) {\\n\\\n",
    "    std::cout << \\\"Exception: \\\" << e << std::endl;\\n\\\n",
    "  }\\n\\\n",
    "  return 0;\\n\\\n",
    "}\\n\\\n",
    "\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
